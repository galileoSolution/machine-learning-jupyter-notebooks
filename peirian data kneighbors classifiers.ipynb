{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K Nearest Neighbors with Python\n",
    "You've been given a classified data set from a company! They've hidden the feature column names but have given you the data and the target classes.\n",
    "We'll try to use KNN to create a model that directly predicts a class for a new data point based off of the features.\n",
    "Let's grab it and use it!\n",
    "\n",
    "Import Libraries\n",
    "In [43]:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "Get the Data\n",
    "Set index_col=0 to use the first column as the index.\n",
    "In [74]:\n",
    "df = pd.read_csv(\"Classified Data\",index_col=0)\n",
    "In [75]:\n",
    "df.head()\n",
    "Out[75]:\n",
    "\n",
    "WTT\n",
    "PTI\n",
    "EQW\n",
    "SBI\n",
    "LQE\n",
    "QWG\n",
    "FDJ\n",
    "PJF\n",
    "HQE\n",
    "NXJ\n",
    "TARGET CLASS\n",
    "0\n",
    "0.913917\n",
    "1.162073\n",
    "0.567946\n",
    "0.755464\n",
    "0.780862\n",
    "0.352608\n",
    "0.759697\n",
    "0.643798\n",
    "0.879422\n",
    "1.231409\n",
    "1\n",
    "1\n",
    "0.635632\n",
    "1.003722\n",
    "0.535342\n",
    "0.825645\n",
    "0.924109\n",
    "0.648450\n",
    "0.675334\n",
    "1.013546\n",
    "0.621552\n",
    "1.492702\n",
    "0\n",
    "2\n",
    "0.721360\n",
    "1.201493\n",
    "0.921990\n",
    "0.855595\n",
    "1.526629\n",
    "0.720781\n",
    "1.626351\n",
    "1.154483\n",
    "0.957877\n",
    "1.285597\n",
    "0\n",
    "3\n",
    "1.234204\n",
    "1.386726\n",
    "0.653046\n",
    "0.825624\n",
    "1.142504\n",
    "0.875128\n",
    "1.409708\n",
    "1.380003\n",
    "1.522692\n",
    "1.153093\n",
    "1\n",
    "4\n",
    "1.279491\n",
    "0.949750\n",
    "0.627280\n",
    "0.668976\n",
    "1.232537\n",
    "0.703727\n",
    "1.115596\n",
    "0.646691\n",
    "1.463812\n",
    "1.419167\n",
    "1\n",
    "\n",
    "Standardize the Variables\n",
    "Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.\n",
    "In [78]:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "In [79]:\n",
    "scaler = StandardScaler()\n",
    "In [80]:\n",
    "scaler.fit(df.drop('TARGET CLASS',axis=1))\n",
    "Out[80]:\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "In [81]:\n",
    "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n",
    "In [82]:\n",
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\n",
    "df_feat.head()\n",
    "Out[82]:\n",
    "\n",
    "WTT\n",
    "PTI\n",
    "EQW\n",
    "SBI\n",
    "LQE\n",
    "QWG\n",
    "FDJ\n",
    "PJF\n",
    "HQE\n",
    "NXJ\n",
    "0\n",
    "-0.123542\n",
    "0.185907\n",
    "-0.913431\n",
    "0.319629\n",
    "-1.033637\n",
    "-2.308375\n",
    "-0.798951\n",
    "-1.482368\n",
    "-0.949719\n",
    "-0.643314\n",
    "1\n",
    "-1.084836\n",
    "-0.430348\n",
    "-1.025313\n",
    "0.625388\n",
    "-0.444847\n",
    "-1.152706\n",
    "-1.129797\n",
    "-0.202240\n",
    "-1.828051\n",
    "0.636759\n",
    "2\n",
    "-0.788702\n",
    "0.339318\n",
    "0.301511\n",
    "0.755873\n",
    "2.031693\n",
    "-0.870156\n",
    "2.599818\n",
    "0.285707\n",
    "-0.682494\n",
    "-0.377850\n",
    "3\n",
    "0.982841\n",
    "1.060193\n",
    "-0.621399\n",
    "0.625299\n",
    "0.452820\n",
    "-0.267220\n",
    "1.750208\n",
    "1.066491\n",
    "1.241325\n",
    "-1.026987\n",
    "4\n",
    "1.139275\n",
    "-0.640392\n",
    "-0.709819\n",
    "-0.057175\n",
    "0.822886\n",
    "-0.936773\n",
    "0.596782\n",
    "-1.472352\n",
    "1.040772\n",
    "0.276510\n",
    "\n",
    "Train Test Split\n",
    "In [83]:\n",
    "from sklearn.model_selection import train_test_split\n",
    "In [84]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],\n",
    "                                                    test_size=0.30)\n",
    "\n",
    "Using KNN\n",
    "Remember that we are trying to come up with a model to predict whether someone will TARGET CLASS or not. We'll start with k=1.\n",
    "In [85]:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "In [86]:\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "In [87]:\n",
    "knn.fit(X_train,y_train)\n",
    "Out[87]:\n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
    "           weights='uniform')\n",
    "In [88]:\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "Predictions and Evaluations\n",
    "Let's evaluate our KNN model!\n",
    "In [89]:\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "In [90]:\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "[[125  18]\n",
    " [ 13 144]]\n",
    "In [91]:\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.91      0.87      0.89       143\n",
    "          1       0.89      0.92      0.90       157\n",
    "\n",
    "avg / total       0.90      0.90      0.90       300\n",
    "\n",
    "\n",
    "Choosing a K Value\n",
    "Let's go ahead and use the elbow method to pick a good K Value:\n",
    "In [98]:\n",
    "error_rate = []\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "In [99]:\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "Out[99]:\n",
    "<matplotlib.text.Text at 0x11ca82ba8>\n",
    "\n",
    " \n",
    "\n",
    "Here we can see that that after arouns K>23 the error rate just tends to hover around 0.06-0.05 Let's retrain the model with that and check the classification report!\n",
    "In [100]:\n",
    "# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('WITH K=1')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "WITH K=1\n",
    "\n",
    "\n",
    "[[125  18]\n",
    " [ 13 144]]\n",
    "\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.91      0.87      0.89       143\n",
    "          1       0.89      0.92      0.90       157\n",
    "\n",
    "avg / total       0.90      0.90      0.90       300\n",
    "\n",
    "In [101]:\n",
    "# NOW WITH K=23\n",
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('WITH K=23')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "WITH K=23\n",
    "\n",
    "\n",
    "[[132  11]\n",
    " [  5 152]]\n",
    "\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.96      0.92      0.94       143\n",
    "          1       0.93      0.97      0.95       157\n",
    "\n",
    "avg / total       0.95      0.95      0.95       300\n",
    "\n",
    "\n",
    "Great job!\n",
    "We were able to squeeze some more performance out of our model by tuning to a better K value!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
